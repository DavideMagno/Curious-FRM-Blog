<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The CuRious Financial Risk ManageR</title>
    <link>/</link>
    <description>Recent content on The CuRious Financial Risk ManageR</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Mar 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Using the tidyverse for swap pricing</title>
      <link>/2019/03/using-the-tidyverse-for-swap-pricing/</link>
      <pubDate>Sun, 31 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/03/using-the-tidyverse-for-swap-pricing/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;I am a big passionate of the &lt;strong&gt;tidyverse&lt;/strong&gt; packages: I think they make the code very clean and clear. In particular, I like the &lt;strong&gt;lubridate&lt;/strong&gt; packages for managing and making operations with dates but its major drawback is that it doesn’t manage financial holidays, which are key when projecting financial cashflows linked to instruments like interest rte swaps.&lt;/p&gt;
&lt;p&gt;In this case, I prefer to use the &lt;strong&gt;RQuantLib&lt;/strong&gt; package.&lt;/p&gt;
&lt;p&gt;I will show how these packages can interact quite well by pricing a simple 7 years forward starting interest rate swap.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-simplified-interest-rate-environment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A simplified interest rate environment&lt;/h3&gt;
&lt;p&gt;Let’s firstly define a &lt;strong&gt;simplified&lt;/strong&gt; market with flat &lt;em&gt;zero rates&lt;/em&gt; at 2.5% (after all, not much different to the USD swap curve at the time of writing this post).&lt;/p&gt;
&lt;p&gt;The discount factors formula can be found &lt;a href=&#34;https://en.wikipedia.org/wiki/Discounting#cite_note-Economics_Discount-2&#34;&gt;here&lt;/a&gt; and it is used as per the code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Today &amp;lt;- lubridate::ymd(20190329) # curve snap date
R &amp;lt;- 0.025 # zero rate
T2M &amp;lt;- seq(0,10) # sequential time to maturity
DF &amp;lt;- 1/(1 + R)^T2M # discount factors
DF_Table &amp;lt;- tibble::tibble(T2M = T2M, DF = DF) # discount factors table&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pricing-a-forward-starting-swap&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pricing a forward starting swap&lt;/h3&gt;
&lt;p&gt;Let’s now consider a forward starting swap that settles on the 31st of May 2019 and that has got 7 years of maturity.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;advance&lt;/code&gt; function in the &lt;strong&gt;RQuantLib&lt;/strong&gt; package allows to calculate a date in the future given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a calendar with the set of holidays well defined&lt;/li&gt;
&lt;li&gt;a starting date&lt;/li&gt;
&lt;li&gt;the number of time units to move forward&lt;/li&gt;
&lt;li&gt;the basic time unit to move forward (days, weeks, months…)&lt;/li&gt;
&lt;li&gt;a business day convention (please refer to the help page of the function for more info &lt;code&gt;?RQuantLib::advance&lt;/code&gt; )&lt;/li&gt;
&lt;li&gt;whether the end-of-month rule applies (ie. if the starting date is the last business date of the month and the time unit is monthly or yearly, the forward date has to be the last date of the target month, net of business day adjustment)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code below shows how to use this function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RQuantLib::advance(calendar = &amp;quot;UnitedStates&amp;quot;, 
                   dates = lubridate::ymd(20190531), 
                   n = 1, 
                   timeUnit = 3, 
                   bdc = 2, 
                   emr = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2020-05-29&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is extremely simple for us to extend this for all the future dates in the next 7 years, using the &lt;em&gt;purrr&lt;/em&gt; function &lt;code&gt;map_dbl&lt;/code&gt;. We also prepend the starting date of the swap.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
Start_Date &amp;lt;- lubridate::ymd(20190531)
Swap_Dates &amp;lt;- purrr::map_dbl(1:7,~RQuantLib::advance(calendar = &amp;quot;UnitedStates&amp;quot;,
                                       dates = Start_Date,
                                       n = .x, 
                                       timeUnit = 3, 
                                       bdc = 2, 
                                       emr = TRUE)) %&amp;gt;% 
  lubridate::as_date() %&amp;gt;% 
  append(Start_Date, ., after = 1)

Swap_Dates&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2019-05-31&amp;quot; &amp;quot;2020-05-29&amp;quot; &amp;quot;2021-05-28&amp;quot; &amp;quot;2022-05-31&amp;quot; &amp;quot;2023-05-31&amp;quot;
## [6] &amp;quot;2024-05-31&amp;quot; &amp;quot;2025-05-30&amp;quot; &amp;quot;2026-05-29&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can immediately see that the &lt;code&gt;advance&lt;/code&gt; function automatically manages the weekends adjusting the final dates.&lt;/p&gt;
&lt;p&gt;We should now convert these dates into time to maturities by using the day count convention ACT/360 (value 0 for the &lt;code&gt;DayCounter&lt;/code&gt; parameter in the function). Again, I use the &lt;strong&gt;RQuantLib&lt;/strong&gt; function &lt;code&gt;yearFraction&lt;/code&gt; in conjunction with the &lt;em&gt;map_dbl&lt;/em&gt; to programatically operate on all the dates as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Swap &amp;lt;- purrr::map_dbl(Swap_Dates, ~RQuantLib::yearFraction(Today, .x, 0)) %&amp;gt;% 
  tibble::tibble(Swap_YF = .)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now interpolate the original discount factor curve linearly over these year fractions. We use the &lt;code&gt;approx&lt;/code&gt; function together with the &lt;code&gt;pluck&lt;/code&gt; one from &lt;strong&gt;purrr&lt;/strong&gt; as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Swap %&amp;lt;&amp;gt;% 
  dplyr::mutate(DF = approx(DF_Table$T2M, DF_Table$DF, .$Swap_YF) %&amp;gt;% 
                  purrr::pluck(&amp;quot;y&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now price the swap rate using the &lt;strong&gt;old fashion&lt;/strong&gt; formula which can be found &lt;a href=&#34;https://en.wikipedia.org/wiki/Interest_rate_swap&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We code the formula in the following function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;OLD_Swap_Rate_Calculation &amp;lt;- function(Swap){
  num &amp;lt;- (Swap$DF[1] - Swap$DF[dim(Swap)[1]])
  annuity &amp;lt;- (sum(diff(Swap$Swap_YF)*Swap$DF[2:dim(Swap)[1]]))
  return(num/annuity)
}

OLD_Swap_Rate_Calculation(Swap)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02500193&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Having the forward exactly equal to 2.5% (net of a small effect due to the ACT/360 day count convention) confirms that the calculation is performed correctly.&lt;/p&gt;
&lt;p&gt;Below you can find the code in its entirety (we using the ACT/ACT day count convention to show that the actual result is equal to 2.5% to 1/1000th of bp):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Interest Rate Setting
Today &amp;lt;- lubridate::ymd(20190329) # curve snap date
R &amp;lt;- 0.025 # zero rate
T2M &amp;lt;- seq(0,10) # sequential time to maturity
DF &amp;lt;- 1/(1 + R)^T2M # discount factors
DF_Table &amp;lt;- tibble::tibble(T2M = T2M, DF = DF) # discount factors table

## Swap Pricing
Swap_Start_Date &amp;lt;- lubridate::ymd(20190531)
Swap_Rate &amp;lt;- purrr::map_dbl(1:7,~RQuantLib::advance(calendar = &amp;quot;UnitedStates&amp;quot;,
                                       dates = Swap_Start_Date,
                                       n = .x, 
                                       timeUnit = 3, 
                                       bdc = 2, 
                                       emr = TRUE)) %&amp;gt;% 
  lubridate::as_date() %&amp;gt;% 
  append(Swap_Start_Date, ., after = 1) %&amp;gt;% 
  purrr::map_dbl(~RQuantLib::yearFraction(Today, .x, 2)) %&amp;gt;% 
  tibble::tibble(Swap_YF = .) %&amp;gt;% 
  dplyr::mutate(DF = approx(DF_Table$T2M, DF_Table$DF, .$Swap_YF) %&amp;gt;% 
                  purrr::pluck(&amp;quot;y&amp;quot;)) %&amp;gt;% 
  OLD_Swap_Rate_Calculation

Swap_Rate&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02499993&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can notice how easy it was to price the swap using only 5 tidyverse functions, 2 RQuantLib ones and a bespoke one!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;spoiler-on-the-next-post&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Spoiler on the next post&lt;/h3&gt;
&lt;p&gt;The function we described will be the starting point for the natural extension of the study:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we will use a real market consistent discount factor curve to test the pricing and&lt;/li&gt;
&lt;li&gt;we will programatically extend the function for it to price potentially an infinite number of contracts at the same time&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>US Federal Spending Analysis</title>
      <link>/2019/02/us-federal-spending-analysis/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/02/us-federal-spending-analysis/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;This week&amp;#39;s &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; is about federal spending. &lt;br&gt;&lt;br&gt;I was interested in understanding if spending had shifted towards clean energy sub-agencies in the last 20 years. Spoiler alert: unfortunately no! 😰&lt;a href=&#34;https://twitter.com/hashtag/Rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/tidyverse?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#tidyverse&lt;/a&gt; cc &lt;a href=&#34;https://twitter.com/thomas_mock?ref_src=twsrc%5Etfw&#34;&gt;@thomas_mock&lt;/a&gt; &lt;a href=&#34;https://twitter.com/R4DScommunity?ref_src=twsrc%5Etfw&#34;&gt;@R4DScommunity&lt;/a&gt; &lt;a href=&#34;https://t.co/AbQzheSagU&#34;&gt;pic.twitter.com/AbQzheSagU&lt;/a&gt;&lt;/p&gt;&amp;mdash; Davide Magno (@DavideMagno) &lt;a href=&#34;https://twitter.com/DavideMagno/status/1095446547091853319?ref_src=twsrc%5Etfw&#34;&gt;February 12, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>Geospatial Analysis on Housing Price Index</title>
      <link>/2019/02/geospatial-analysis-on-housing-price-index/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/02/geospatial-analysis-on-housing-price-index/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Here&amp;#39;s my &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; submission.I studied the total increase in the housing index for each US State compared to the US federal index. No surprise in California house price has increased much more than the rest of US.The &lt;a href=&#34;https://twitter.com/hashtag/mapdata?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#mapdata&lt;/a&gt; library was 👍🏼&lt;a href=&#34;https://twitter.com/thomas_mock?ref_src=twsrc%5Etfw&#34;&gt;@thomas_mock&lt;/a&gt; &lt;a href=&#34;https://twitter.com/R4DScommunity?ref_src=twsrc%5Etfw&#34;&gt;@R4DScommunity&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://t.co/sYI1zKhIpC&#34;&gt;pic.twitter.com/sYI1zKhIpC&lt;/a&gt;&lt;/p&gt;&amp;mdash; Davide Magno (@DavideMagno) &lt;a href=&#34;https://twitter.com/DavideMagno/status/1094707858367758337?ref_src=twsrc%5Etfw&#34;&gt;February 10, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>My First #TidyTuesday Submission</title>
      <link>/2019/02/my-first-tidytuesday-submission/</link>
      <pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/02/my-first-tidytuesday-submission/</guid>
      <description>&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Finally my first &lt;a href=&#34;https://twitter.com/hashtag/TidyTuesday?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#TidyTuesday&lt;/a&gt; submission! 🎊🎉I am starting from week 1 2019 but I will catch up soon😉When do people submit their visualisations?Mostly on Tuesday and Wednesday afternoon, but also on Monday before the new set is released &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/ggplot?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#ggplot&lt;/a&gt; &lt;a href=&#34;https://twitter.com/rstats4ds?ref_src=twsrc%5Etfw&#34;&gt;@rstats4ds&lt;/a&gt; &lt;a href=&#34;https://twitter.com/thomas_mock?ref_src=twsrc%5Etfw&#34;&gt;@thomas_mock&lt;/a&gt; &lt;a href=&#34;https://t.co/pKZAyL8vp1&#34;&gt;pic.twitter.com/pKZAyL8vp1&lt;/a&gt;&lt;/p&gt;&amp;mdash; Davide Magno (@DavideMagno) &lt;a href=&#34;https://twitter.com/DavideMagno/status/1092142256373620739?ref_src=twsrc%5Etfw&#34;&gt;February 3, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

</description>
    </item>
    
    <item>
      <title>Convert foreign currency valuations</title>
      <link>/2018/02/convert-foreign-currency-valuations/</link>
      <pubDate>Wed, 14 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/convert-foreign-currency-valuations/</guid>
      <description>


&lt;p&gt;One of the most common problems when dealign with financial data is to have assets (or liabilities) denominated in a currency that is different from the domestic one.&lt;/p&gt;
&lt;p&gt;I propose a tidy solution to this problem that involves no use of for cycles.&lt;/p&gt;
&lt;p&gt;The principle of the solution is that converting each currency can be done in parallel using the &lt;strong&gt;map&lt;/strong&gt; function while the consolidation of the results will be done using the &lt;strong&gt;reduce&lt;/strong&gt; logic.&lt;/p&gt;
&lt;p&gt;I am a big fan of the tidyverse and therefore I will start loading the required packages&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
library(stringr)
library(tibble)
library(magrittr)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I suppose to have a very simple investment dataset made of two fields:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a column with the currency of the investment (local currency being &lt;em&gt;EUR&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;a column with the market value of the investment in local currency&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Investments &amp;lt;- tibble(ccy = c(rep(&amp;quot;EUR&amp;quot;,2), rep(&amp;quot;JPY&amp;quot;,3), rep(&amp;quot;GBP&amp;quot;,3)),
                      local_MV = c(1.5e6,1.2e6,2e8,1.5e8,3e8,1e6,1.5e6,2e6)) %&amp;gt;% 
  mutate(local_MV = as.double(local_MV)) %&amp;gt;% 
  mutate(EUR_MV = local_MV)

Investments&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 3
##   ccy    local_MV    EUR_MV
##   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1 EUR     1500000   1500000
## 2 EUR     1200000   1200000
## 3 JPY   200000000 200000000
## 4 JPY   150000000 150000000
## 5 JPY   300000000 300000000
## 6 GBP     1000000   1000000
## 7 GBP     1500000   1500000
## 8 GBP     2000000   2000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then store in a list the current currency FX rates&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;FX &amp;lt;- list(&amp;quot;JPY&amp;quot; = 130, &amp;quot;GBP&amp;quot; = 0.8)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first part of the algorithm consists in finding the rows of the Investments database which are subject to FX conversion for each of the currencies that are stored in the &lt;strong&gt;FX&lt;/strong&gt; list using the &lt;em&gt;stringr&lt;/em&gt; function &lt;em&gt;str_which&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;This is the “parallel” part of the algorithm because it can be done currency by currency independently. We therefore use the &lt;em&gt;map&lt;/em&gt; function of the &lt;em&gt;purrr&lt;/em&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pos_ccy &amp;lt;- map(names(FX), ~str_which(Investments$ccy,.)) %&amp;gt;% 
  set_names(names(FX))

pos_ccy&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $JPY
## [1] 3 4 5
## 
## $GBP
## [1] 6 7 8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This list contains the row numbers for which there is an investment denominated in each of the currencies in the FX universe.
We then use this information together with the actual FX rate to convert the investments’ local market value. We now use the &lt;em&gt;map2&lt;/em&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;converted_MV &amp;lt;- map2(pos_ccy, FX ,~Investments$local_MV[.x]/.y)

converted_MV&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $JPY
## [1] 1538462 1153846 2307692
## 
## $GBP
## [1] 1250000 1875000 2500000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can notice that we now have a list made by two different vectors of converted market values. We need to consolidate those into the EUR_MV column in the Investments dataset.&lt;/p&gt;
&lt;p&gt;This is the second step of the algorithm that uses the &lt;em&gt;reduce&lt;/em&gt; function of &lt;em&gt;purrr&lt;/em&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Investments$EUR_MV[pos_ccy %&amp;gt;% reduce(c)] &amp;lt;- converted_MV %&amp;gt;% 
  reduce(c)

Investments&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 8 x 3
##   ccy    local_MV   EUR_MV
##   &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 EUR     1500000 1500000 
## 2 EUR     1200000 1200000 
## 3 JPY   200000000 1538462.
## 4 JPY   150000000 1153846.
## 5 JPY   300000000 2307692.
## 6 GBP     1000000 1250000 
## 7 GBP     1500000 1875000 
## 8 GBP     2000000 2500000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function modifies rows of the EUR_MV field by reducing the lists from the map functions into vectors.&lt;/p&gt;
&lt;p&gt;We can notice that the EUR positions have not changed their market value.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generate scenarios correlated to existing ones</title>
      <link>/2018/01/generate-scenarios-correlated-to-existing-ones/</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/generate-scenarios-correlated-to-existing-ones/</guid>
      <description>


&lt;p&gt;In quantitative finance we often look at simulations of some market risk factors like equity returns or interest rate changes.&lt;/p&gt;
&lt;p&gt;There are many third party companies who specialize in the historical calibration of such variables and provide simulations of future expected outcomes to the companies who require them.&lt;/p&gt;
&lt;p&gt;For example, let’s suppose that we receive the expected returns of the Google shares as per the following distribution&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This modelling is given by the third party and in theory we don&amp;#39;t know it
google &amp;lt;- rnorm(10000, mean = 0.01, sd = 0.20) %&amp;gt;% 
  tibble(google_returns = .)

ggplot(data = google, aes(x = google_returns)) + 
  geom_density() +
  scale_x_continuous(labels = percent) + 
  labs(title = &amp;quot;Google Shares&amp;quot;,
        subtitle = &amp;quot;One Year forward distribution&amp;quot;) +
  xlab(&amp;quot;One year forward return&amp;quot;) + 
  ylab(&amp;quot;Density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/22_01_2018_simulations_files/figure-html/Google-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now want to simulate the distribution of another risk factor which is not provided by the third party but that is usefull for us. Let’s say it is the distribution of the Facebook shares which we imagine to:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;be distributed as a normal random variable&lt;/li&gt;
&lt;li&gt;have a 5% expected return and&lt;/li&gt;
&lt;li&gt;being quite volatile (30% annual volatility)&lt;/li&gt;
&lt;li&gt;have the same number of points as the simulations provided by the third party (10000 points)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We model therefore the distribution as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This modelling is our internal one and we can therefore control it
fb &amp;lt;- rnorm(10000, mean = 0.05, sd = 0.30) %&amp;gt;% 
  tibble(fb_returns = .)

ggplot(data = fb, aes(x = fb_returns)) + 
  geom_density() +
  scale_x_continuous(labels = percent) + 
  labs(title = &amp;quot;Facebook Shares&amp;quot;,
        subtitle = &amp;quot;One Year forward distribution&amp;quot;) +
  xlab(&amp;quot;One year forward return&amp;quot;) + 
  ylab(&amp;quot;Density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/22_01_2018_simulations_files/figure-html/Facebook-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s now look at the correlation between the two variables:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Returns &amp;lt;- google %&amp;gt;% 
  cbind(fb) %&amp;gt;% 
  as.tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `as.tibble()` is deprecated, use `as_tibble()` (but mind the new semantics).
## This warning is displayed once per session.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(Returns$fb_returns, Returns$google_returns)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.0166395&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/22_01_2018_simulations_files/figure-html/Correlation_graph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can notice that performing the simple simulation doesn’t allow us to impose a correlation structure with the data provided by the third party provider.&lt;/p&gt;
&lt;p&gt;How can we generate a random variable with a defined correlation to an existing one?
A very elegant solution was provided by user &lt;strong&gt;whuber&lt;/strong&gt; at the following link &lt;a href=&#34;https://stats.stackexchange.com/a/313138&#34; class=&#34;uri&#34;&gt;https://stats.stackexchange.com/a/313138&lt;/a&gt; and by using the following function&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;complement &amp;lt;- function(y, rho, x) {
  if (missing(x)) x &amp;lt;- rnorm(length(y)) # Optional: supply a default if `x` is not given
  y.perp &amp;lt;- residuals(lm(x ~ y))
  rho * sd(y.perp) * y + y.perp * sd(y) * sqrt(1 - rho^2)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this function &lt;em&gt;x&lt;/em&gt; is our Facebook uncorrelated scenario , &lt;em&gt;rho&lt;/em&gt; is the correlation level and &lt;em&gt;y&lt;/em&gt; is the Google scenario as provided by the third party.&lt;/p&gt;
&lt;p&gt;We apply this function imposing an 80% correlation&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Returns %&amp;lt;&amp;gt;%  
  mutate(fb_correlated = complement(.$google_returns,0.8, .$fb_returns))

cor(Returns$google_returns, Returns$fb_correlated)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/22_01_2018_simulations_files/figure-html/Add_Correlation_Graph-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is just one last thing to do: compare the distributions of the two Facebook simulations&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Returns %&amp;gt;% 
  select(contains(&amp;quot;fb&amp;quot;)) %&amp;gt;% 
  gather(simulation_type, simulation_value) %&amp;gt;% 
  ggplot(aes(x = simulation_value, colour = simulation_type)) +
  geom_density() +
  scale_x_continuous(labels = percent) + 
  labs(title = &amp;quot;Facebook Shares&amp;#39; simulations&amp;quot;) +
  xlab(&amp;quot;One year forward return&amp;quot;) + 
  ylab(&amp;quot;Density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/22_01_2018_simulations_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can notice that the marginal distribution of the correlated scenario is much different from the original one.&lt;/p&gt;
&lt;p&gt;There is therefore one additional step before we finish: we need to rescale the marginal distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Rescaled_fb_correlated &amp;lt;- Returns %&amp;gt;%
  select(contains(&amp;quot;correlated&amp;quot;)) %&amp;gt;%
  scale() %&amp;gt;% 
  multiply_by(0.3) %&amp;gt;% 
  add(0.05)

Returns %&amp;lt;&amp;gt;% 
  mutate(fb_correlated_scaled = Rescaled_fb_correlated)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now check that the the marginals are comparable and that the correlation structure is still at the desired level&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Returns %&amp;gt;% 
  select(contains(&amp;quot;fb&amp;quot;)) %&amp;gt;% 
  gather(simulation_type, simulation_value) %&amp;gt;% 
  ggplot(aes(x = simulation_value, colour = simulation_type)) +
  geom_density() +
  scale_x_continuous(labels = percent) + 
  labs(title = &amp;quot;Facebook Shares&amp;#39; simulations&amp;quot;) +
  xlab(&amp;quot;One year forward return&amp;quot;) + 
  ylab(&amp;quot;Density&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/22_01_2018_simulations_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cor(Returns$google_returns, Returns$fb_correlated)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new Facebook scenario is now both correlated and in line with the density we want.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
